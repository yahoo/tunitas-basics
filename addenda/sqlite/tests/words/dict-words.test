#!/bin/sh
f=${0##*/}
b=${f%.test}
cd "${0%/*}"
export PATH="../../check/bin:../../bin:$PATH"
default_wordsfile=/usr/share/dict/words
# on a fast filesystem (avoid using NFS)
wordsfile=${TMPDIR:-/tmp}/t99.dict-words.list
if [ -f $wordsfile ] ; then
    # it already exists
    unset cleanup_wordsfile
elif [ -f $default_wordsfile ] ; then
    # symlink to the big database of words
    cleanup_wordsfile=$wordsfile
    ln -s $default_wordsfile $wordsfile
else
    # else make up a database of fake words
    cleanup_wordsfile=$wordsfile
    uu=$(uuidgen)
    for n in {0..450000} ; do
        echo "$uu-$n"
    done > $wordsfile
fi
dbfile=${TMPDIR:-/tmp}/$b.sqlite
function cleanup() {
    local reason=$1
    if [ -z "$reason" ] ; then
        reason=finally
    fi
    echo "cleaning up $reason, removing $dbfile"
    rm -f $dbfile
    rm -f $cleanup_wordsfile
}
trap cleanup EXIT

# DO NOT - this will remove the $wordlist file
# DO NOT 
#     cleanup initially

if false ; then
    echo "FAIL ... this test takes too long  Why?"
    exit 1
fi

# keep it bounded ... else we will be here all day
ls -ld $wordsfile
words --verbose --row-insertion-limit=200 --database=$dbfile $wordsfile
